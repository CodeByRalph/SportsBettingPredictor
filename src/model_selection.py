import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib

# feature imports
from src.features.other_off_features.yds import add_yds_features

# model imports
from src.models.random_forest import train_random_forest
from src.models.xgboost import train_xgboost


def preprocess_data(df, target_stat):
    """
    Preprocesses the data for training, including feature engineering and normalization.
    
    Args:
        df (pd.DataFrame): The cleaned dataset.
    
    Returns:
        X_train, X_test, y_train, y_test: Processed training and test sets.
        scaler: The scaler object used for normalization.
    """
    # Removes Data Column but saves it for use in visualization
    date_col = df['Date']

    df['Date'] = pd.to_datetime(df['Date'])

    # Extract date-related features
    #df['day_of_week'] = df['Date'].dt.dayofweek  # Day of the week (Monday = 0, Sunday = 6)         # Month of the game
    df['is_weekend'] = df['Date'].dt.dayofweek >= 5  # Boolean for weekend games
    df['days_since_last_game'] = (df['Date'] - df['Date'].shift(1)).dt.days.fillna(0)  # Time gap between games

    df = add_yds_features(df)

    # Drop irrelevant columns
    df = df.drop(['Date'], axis=1)
            
    # Drop rows with NaN values generated by rolling and shifting operations
    df = df.dropna()

    # Target variable and feature columns
    X = df.drop(target_stat, axis=1)
    y = df[target_stat]

    # # Check for infinite values in the dataframe
    # print("Checking for infinite values in the dataframe:")
    # print(np.isinf(X).sum())

    # # Replace inf with a large value or drop rows with inf
    # X.replace([np.inf, -np.inf], np.nan, inplace=True)
    # X = X.dropna()

    # print("Data after replacing infinities:")
    # print(X.head())

    mean = y.mean()

    feature_names = X.columns

     # Log transformation for the YDS stat
    if target_stat == 'YDS' or target_stat == 'YDS2':
        y = np.log1p(y)

    # Normalize the features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Debugging RFE step
    try:
        rfe_model = RandomForestRegressor()
        rfe = RFE(estimator=rfe_model, n_features_to_select=10)
        X_rfe = rfe.fit_transform(X_scaled, y)
    except Exception as e:
        print(f"Error in RFE: {e}")


    # Get the feature names that were selected by RFE
    selected_feature_names = feature_names[rfe.support_]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.3, shuffle=False, random_state=42)

     # Debugging: Print feature names and shapes before fitting
    # print(f"Number of features in X_train: {X_train.shape[1]}")
    # print(f"Number of features in X_test: {X_test.shape[1]}")
    # print(f"Feature names: {X.columns}")

    return X_train, X_test, y_train, y_test, scaler, date_col, mean, rfe, selected_feature_names


def predict_next_game_for_all_models(models, X_last_game, scaler, target_stat):
    """
    Predicts the target stat for the next game using the trained models.

    Args:
        models (dict): Dictionary of trained models.
        X_last_game (np.array): The feature data of the last game to predict the next game.
    """
    print("\n---------------------------------------")
    print("Predicting next game's target stat for each model:")

    X_next_game = X_last_game

    for model_name, model in models.items():
        try:
            # Predict the target stat for the next game
            y_pred = model.predict(X_last_game)

            if target_stat == 'YDS':
                y_pred = np.expm1(y_pred)

            print(f"{model_name} prediction for next game: {np.round(y_pred[0])}")
        except Exception as e:
            print(f"Error in {model_name} during prediction: {e}")
    print("\n---------------------------------------")


def evaluate_model(model, X_test, y_test, target_stat, mean):
    try:
        y_pred = model.predict(X_test)

        # Revert log transformation for YDS
        if target_stat == 'YDS':
            y_pred = np.expm1(y_pred)
            y_test = np.expm1(y_test)

        print(f"Predictions:   {np.round(y_pred).astype(int)}")
        print(f"Actual values: {np.round(y_test.values).astype(int)}")

        # Ensure the lengths of y_pred and y_test match
        if len(y_pred) != len(y_test):
            raise ValueError(f"Length mismatch: y_pred has {len(y_pred)} elements, y_test has {len(y_test)} elements")

        try:
            accuracy = accuracy_score(y_test.astype(int), np.round(y_pred).astype(int))
            print(f"\nAccuracy: {accuracy * 100:.2f}%")
        except Exception as e:
            print(f"Error calculating Accuracy_score: {e}")

        # Calculate Mean Squared Error
        try:
            mse = mean_squared_error(y_test, y_pred)
            print(f"Mean Squared Error (MSE): {mse}")
        except Exception as e:
            print(f"Error calculating MSE: {e}")

        # Calculate Root Mean Squared Error (optional)
        try:
            rmse = np.sqrt(mse)
            print(f"Root Mean Squared Error (RMSE): {rmse}")
            print("Percent Error: ", (rmse/mean) * 100)

        except Exception as e:
            print(f"Error calculating RMSE: {e}")

        # Calculate R² score (optional)
        try:
            r2 = r2_score(y_test, y_pred)
            print(f"R² Score: {r2}")
        except Exception as e:
            print(f"Error calculating R² score: {e}")
    except Exception as e:
        print(f"Error occurred during model evaluation: {e}")


def test_model_performance(player_id, target_stat):
    data_file_path = os.path.join('data', 'processed', f'player_{player_id}_clean.csv')
    df = pd.read_csv(data_file_path)
    
    X_train, X_test, y_train, y_test, scaler, date_col, mean, rfe, selected_feature_names = preprocess_data(df, target_stat)

    # Train and evaluate multiple models
    models = {
        'RandomForest': train_random_forest(X_train, y_train),
        'XGBoost': train_xgboost(X_train, y_train),
    }


    for model_name, model in models.items():
        print('\n---------------------------------------')
        print(f"Evaluating {model_name}...")
        evaluate_model(model, X_test, y_test, target_stat, mean)
    
        if hasattr(model, 'feature_importances_'):
            print(f"Plotting feature importances for {model_name}")
            
            # Get the feature importances and sort them
            importances = model.feature_importances_
            sorted_indices = np.argsort(importances)[::-1]
            # Select the top N indices
            top_indices = sorted_indices[:25]

            # Use these indices to get the top N feature names and their importances
            top_features = pd.DataFrame({
                'Feature': np.array(selected_feature_names)[top_indices],
                'Importance': importances[top_indices]
            })

            # Create a horizontal bar chart
            plt.figure(figsize=(10, 6))
            plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')
            plt.xlabel('Importance')
            plt.ylabel('Feature')
            plt.title(f'Feature Importance for {model_name}')
            plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top
            plt.tight_layout()

            # Save the plot to a file instead of showing it
            output_path = f"feature_importance_{model_name}.png"
            plt.savefig(output_path)
            print(f"Feature importance plot saved as {output_path}")

            plt.close() 

    # Predict the next game using the average of the last 3 games
    X_next_game = np.mean(X_test[-5:], axis=0).reshape(1, -1) 
    predict_next_game_for_all_models(models, X_next_game, scaler, target_stat)